import os.path as osp
import pickle

import gym
import numpy as np
import sapien.core as sapien
from mani_skill2.utils.common import register_gym_env
from mani_skill2.utils.wrappers import (ManiSkillActionWrapper,
                                        NormalizeActionWrapper)
from sapien.core import Pose

from tr2.envs.blockstacking.env import (BlockStackFloatPandaEnv,
                                                     BlockStackMagicPandaEnv,
                                                     BlockStackPandaEnv)
from tr2.envs.trajectory_env import TrajectoryEnv
from tr2.envs.world_building import add_target
from tr2.utils.pose_est import predict_pose, predict_pose_noseg


@register_gym_env("BlockStackTrajectory-v0")
class BlockStackTrajectory(TrajectoryEnv):
    def __init__(
        self,
        trajectories=[],
        max_trajectory_length=100,
        max_ep_len_factor=2,
        trajectories_dataset=None,
        stack_size=1,
        fixed_max_ep_len=200,
        max_trajectory_skip_steps=15,
        give_traj_id=False,
        reward_type="original",
        task_agnostic=True,
        trajectory_sample_skip_steps=0,
        randomize_trajectories=True,
        max_rot_stray_dist=0.3,
        max_world_state_stray_dist=0.025,
        max_coord_stray_dist=0.02,
        early_success=False,
        sim_world_noise=None,
        controller="arm",
        goal="pick_and_place_train",
        partial_trajectories=False,
        show_goal_visuals=True,
        obs_mode="state",
        task_range='small',
        intervene_count=0,
        seed_by_dataset=True,
        spawn_all_blocks=False,
        reset_agent_after_trajectory=False,
        real_pose_est=False,
        planner_cfg = dict(
            planner=None,
            planning_env=None,
            render_plan = False,
            max_plan_length=64,
            save_plan_videos=False,
            min_student_execute_length=64,
            re_center=False
        ),
        sub_goals = False,
        **kwargs,
    ):
        """
        partial_trajectories : bool
            if True, assumes the given dataset consists of multipled pre-generated abstract trajectories. As soon as one abstract trajectory is followed to completion, the code will
            allow the agent to start using the next abstract trajectory as well as attend to the corresponding block.

            if False, assumes only one abstract trajectory is given and if multiple are to be used, they are to be generated by a given planner
        """


        # assert len(trajectories) > 0
        self.sim_world_noise = sim_world_noise
        self.reward_type = reward_type
        self.max_rot_stray_dist = max_rot_stray_dist
        self.max_world_state_stray_dist = max_world_state_stray_dist
        self.max_coord_stray_dist = max_coord_stray_dist
        self.plan_id = 0
        self.partial_trajectories = partial_trajectories
        self.obs_mode = obs_mode
        reward_mode = "sparse"
        self.reset_agent_after_trajectory = reset_agent_after_trajectory
        self.goal = goal
        skip_long_trajectories = False
        if reward_type == "lcs_dense" or reward_type == "traj_dense" or reward_type == "traj_dense_far" or reward_type =="dense" or "lcs" in reward_type:
            reward_mode = "dense"
        if goal == "pick_and_place_train":
            env = gym.make(
                "BlockStackArm-v0",
                obs_mode='state_dict',
                reward_mode=reward_mode,
                num_blocks=1,
                controller=controller,
                goal='pick_and_place_train',
                show_goal_visuals=show_goal_visuals,
                task_range=task_range,
            )
        elif goal == 'pick_and_place_silo':
            env = gym.make(
                "BlockStackArm-v0",
                obs_mode='state_dict',
                reward_mode=reward_mode,
                num_blocks=1,
                controller=controller,
                goal='pick_and_place_silo',
                show_goal_visuals=show_goal_visuals,
                task_range=task_range,
            )
        else:
            skip_long_trajectories = True
            env = gym.make(
                "BlockStackArm-v0",
                obs_mode='state_dict',
                reward_mode=reward_mode,
                num_blocks=-1,
                goal=goal,
                controller=controller,
                show_goal_visuals=show_goal_visuals,
                intervene_count=intervene_count,
                spawn_all_blocks=spawn_all_blocks,
                real_pose_est=real_pose_est,
                # TODO add task_range
            )
        env = ManiSkillActionWrapper(env)
        env = NormalizeActionWrapper(env)
        self.grasp_count = 0
        self.match_traj_count = 0
        self.match_traj_frac = 0

        if trajectories_dataset is not None:
            if osp.splitext(trajectories_dataset)[1] == ".npy":
                raw_dataset  = np.load(trajectories_dataset, allow_pickle=True).reshape(1)[0]
            elif osp.splitext(trajectories_dataset)[1] == ".pkl":
                with open(trajectories_dataset, "rb") as f:
                    raw_dataset = pickle.load(f)
            self.raw_dataset = raw_dataset
        if self.obs_mode == "visual":
            state_dims = 25 + 12288
        else:
            state_dims = 32
        super().__init__(
            env=env,
            state_dims=state_dims + (10 if sub_goals else 0), #[9] [9] [7] [3] [4] [3] [3]
            act_dims=env.action_space.shape[0],
            teacher_dims=10, #[5] [5] [7]
            trajectories=trajectories,
            max_trajectory_length=max_trajectory_length,
            trajectories_dataset=trajectories_dataset,
            max_ep_len_factor=max_ep_len_factor,
            stack_size=stack_size,
            fixed_max_ep_len=fixed_max_ep_len,
            give_traj_id=give_traj_id,
            trajectory_sample_skip_steps=trajectory_sample_skip_steps,
            task_agnostic=task_agnostic,
            randomize_trajectories=randomize_trajectories,
            max_trajectory_skip_steps=max_trajectory_skip_steps,
            planner_cfg=planner_cfg,
            early_success=early_success,
            seed_by_dataset=seed_by_dataset,
            sub_goals=sub_goals,
            skip_long_trajectories=skip_long_trajectories
        )
        self.orig_observation_space = self.env.observation_space
        self.teacher_balls = []
    def reset_env(self, seed=None, **kwargs):
        if self.env.intervene_count > 0:
            self.env.allow_next_stage(False)
        if seed is None:
            self.env.reset()
            return
        self.env.seed(seed)
        self.env.reset()
    def reset(self, *args, **kwargs):
        self.env: BlockStackPandaEnv
        self.prev_ob_poses = {}
        self.last_block_coord=None
        self.grasp_count = 0
        self.match_traj_count = 0
        self.match_traj_frac = 0
        self.abstract_part = 0
        
        super_reset = super().reset(*args, **kwargs)
        return super_reset

    def format_ret(self, obs, ret):
        if self.reward_type == "original" or self.reward_type == "dense":
            return ret
        elif self.reward_type == "trajectory":
            reward = 0
            reward += ret / 5
            if self.improved_farthest_traj_step:
                look_ahead_idx = int(min(self.farthest_traj_step, len(self.teacher_student_coord_diff) - 1))
                prog_frac = self.farthest_traj_step / (len(self.teacher_student_coord_diff) - 1)
                dist_to_next = self.weighted_traj_diffs[look_ahead_idx]
                reward += (10 + 50*prog_frac) * (1 - np.tanh(dist_to_next*30))
            else:
                if self.farthest_traj_step < len(self.teacher_student_coord_diff) - 1:
                    reward -= 0 # add step wise penalty if agent hasn't reached the end yet
                else:
                    dist_to_next = self.weighted_traj_diffs[-1]
                    reward += 10 * (1- np.tanh(dist_to_next*30)) # add big reward if agent did reach end and encourage agent to stay close to end
            return reward
        else:
            raise NotImplementedError()


    def format_obs(self, obs):
        obs, _ =super().format_obs(obs)
        obs_dict = obs['observation']
        # 9 qpos, 9 qvel, 7 block pose, 3 EE xyz, 4 EE quat
        dense_obs=np.zeros(32)
        dense_obs[:9] = obs_dict['agent']['qpos']
        #('obs',obs_dict['agent']['qpos'])
        dense_obs[9:18] = obs_dict['agent']['qvel']
        ## there is only one block
        if self.obs_mode == 'state':
            dense_obs[18:25] = obs_dict['extra']['obs_blocks']['absolute'][self.abstract_part*7: (self.abstract_part+1)*7]
            if self.sim_world_noise is not None:
                dense_obs[18:25] += np.random.normal(0.0, self.sim_world_noise, size=(7,))
        student_panda_hand = None
        for link in self.env.get_articulations()[0].get_links():
            if link.name == 'panda_hand':
                student_panda_hand = link
        student_hand_xyz=student_panda_hand.get_pose().p
        student_hand_quat = student_panda_hand.get_pose().q
        dense_obs[25:28] = student_hand_xyz
        dense_obs[28:32] = student_hand_quat
        teacher_agent_xyz = self.orig_trajectory_observations[:, :3]
        teacher_world_states= self.orig_trajectory_observations[:, -7:-4]
        self.teacher_student_coord_diff = np.linalg.norm(
            teacher_agent_xyz - student_hand_xyz , axis=1
        )
        if self.partial_trajectories == False:
            self.abstract_part = self.env.cur_block_idx
        ## REAL WORLD RESCALE
        # dense_obs[7:9] *= 0.01/0.04

        # simulate visual inputs and apply object detection
        obj_xyz = dense_obs[18:21]
        agent_xyz =student_panda_hand.get_pose().p
        if self.obs_mode == "state_visual":
            if np.linalg.norm(agent_xyz - obj_xyz) < 0.11:
                pass #print("### block is in visual range probably, use GT pose")
            else:
                vis = self.env.render("state_visual")
                rgb = vis["rgb"]
                depth = vis["depth"][:,:,0]
                cam_int = vis["camera_intrinsic"]
                cam_ext = vis["camera_extrinsic_world_frame"]
                seg = vis['actor_seg']
                try:
                    ob_poses = predict_pose_noseg(rgb, depth, dict(cam_int=cam_int,cam_ext=cam_ext,object_ids=[self.abstract_part+2]), 
                    min_clusters=max(1,len(self.prev_ob_poses)-1))
                except:
                    print("FAILED OB POSES", self.curr_traj_id)
                    ob_poses = self.prev_ob_poses # {"1":np.array([0,0,0,1,0,0,0])}
                # gt_ob_pose = predict_pose(rgb, depth, seg, dict(cam_int=cam_int,cam_ext=cam_ext,object_ids=[self.abstract_part+2]))
                self.prev_ob_poses=ob_poses
                closest_pose_dist = 99
                for p in ob_poses.values():
                    p[0] -= 0.01 # fix a bias seen in training due to icp
                    teacher_frame = self.orig_trajectory_observations[0]    
                    # dd=np.linalg.norm(dense_obs[18:25][:3] - p[:3])            
                    # dist = (self.teacher_student_coord_diff + np.linalg.norm((teacher_world_states - p[:3]))).min()#np.linalg.norm(teacher_frame[3:6] - p[:3])
                    if self.last_block_coord is None:
                        dist = np.linalg.norm(teacher_frame[3:6] - p[:3])
                    else:
                        dist = np.linalg.norm(self.last_block_coord - p[:3])
                    if dist < closest_pose_dist:
                        closest_pose_dist = dist
                        ob_pose = p
                # FIX ROTATION
                # ob_pose[3:] = np.array([1, 0, 0, 0])
                # ob_pose =predict_pose(rgb, depth, seg, dict(cam_int=cam_int,cam_ext=cam_ext,object_ids=[self.abstract_part+2]))
                diff = np.linalg.norm(ob_pose - dense_obs[18:25]) 
                dense_obs[18:25] = ob_pose
        elif self.obs_mode == "visual":
            vis = self.env.render("state_visual")
            rgb = vis["rgb"].transpose(2,0,1)
            dense_obs = np.hstack([dense_obs[:18], dense_obs[25:32], rgb.flatten()])
        obs['observation'] = dense_obs[:32]
        student_world_state = dense_obs[18:25][:3]
        student_cur_rot = student_panda_hand.get_pose().q
        assert len(student_hand_xyz) == 3
        assert len(student_world_state) == 3
        
        self.last_block_coord = student_world_state
        student_original_rot = np.array([5.63341e-09, 0.997977, 7.54953e-08, 0.0635815])

        

        self.teacher_student_world_diff = np.linalg.norm(
            teacher_world_states - student_world_state, axis=1
        )

        # measure rot, range from 1 (180 deg diff) to 0 (same)
        self.rot_deviated_from_init = 1 - np.inner(student_cur_rot, student_original_rot)**2
        grasped = self.env.agent.check_grasp(self.env.blocks[0])
        if grasped:
            self.grasp_count += 1


        agent_within_dist = self.teacher_student_coord_diff < self.max_coord_stray_dist
        world_within_dist = self.teacher_student_world_diff < self.max_world_state_stray_dist
        idxs = np.where(world_within_dist & agent_within_dist)[0] # determine which teacher frames are within distance

        self.weighted_traj_diffs = self.teacher_student_coord_diff * 0.1 + self.teacher_student_world_diff * 0.9
        self.closest_traj_step = 0
        self.improved_farthest_traj_step = False
        if self.rot_deviated_from_init < self.max_rot_stray_dist and len(idxs) > 0:
            closest_traj_step = idxs[(self.weighted_traj_diffs)[idxs].argmin()]
            if closest_traj_step - self.farthest_traj_step < self.max_trajectory_skip_steps:
                self.closest_traj_step = closest_traj_step
                if closest_traj_step > self.farthest_traj_step:
                    self.improved_farthest_traj_step = True
                    self.match_traj_count += 1
                self.farthest_traj_step = max(closest_traj_step, self.farthest_traj_step)
        
        
        max_world_diff = 0.05
        if self.env.real_pose_est:
            max_world_diff = 0.08
        if self.teacher_student_coord_diff[-1] < 0.05 and self.teacher_student_world_diff[-1] < max_world_diff: # needs to align with block stack planner threshold
            if "train" not in self.goal:
                self.clear_past_obs()
                self.abstract_part += 1
                self.last_block_coord = None
               
                # following is not necessary, only use for models that don't learn to return to a resting position (which is not our case)
                if self.reset_agent_after_trajectory:
                    qpos = np.array(
                        [0, np.pi / 16, 0, -np.pi * 5 / 6, 0, np.pi - 0.2, np.pi / 4, 0, 0]
                    )
                    # useful for real world experimentation and dealing with learned models that
                    # enter unsupported regions if they don't return to a good rest point
                    self.env.agent.reset(qpos)
                    self.env.agent._robot.set_pose(Pose([-0.56, 0, 0]))
                    if self.env.intervene_count > 0:
                        self.env.allow_next_stage(True)
            if self.partial_trajectories:
                # if agent reaches the end and is close enough, we continue to next steps
                traj = self.trajectories[self.curr_traj_idx].copy()
                traj["observations"] = traj["observations"][np.where(traj["attns"] == self.abstract_part)]
                if len(traj['observations']) > 0:
                    self._select_trajectory(traj)
                else:
                    self.abstract_part -= 1
            self.abstract_part = min(self.abstract_part, len(self.env.blocks) - 1)
        else:
            if self.env.intervene_count > 0:
                self.env.allow_next_stage(False)
        return obs, {
            "stats": {
                "grasp": self.grasp_count,
                "match_traj_count": self.match_traj_count,
                "match_traj_frac": self.match_traj_count / (len(self.weighted_traj_diffs) - 1)
            }
        }
    def next_sub_goal(self):
        return self.orig_trajectory_observations[min(self.farthest_traj_step + 10, len(self.orig_trajectory_observations) - 1)]

    def get_trajectory(self, t_idx):
        trajectory=self.raw_dataset["teacher"][str(t_idx)]
        if self.sim_world_noise is not None:
            trajectory["observations"][:, -7:] += np.random.normal(0.0, self.sim_world_noise, size=(len(trajectory["observations"]), 7))
        return trajectory

    def seed(self, seed, *args, **kwargs):
        self.env.seed(seed)
        return super().seed(seed, *args, **kwargs)

    def get_obs(self):
        return self.env.get_obs()

    def get_state(self):
        return self.env.get_state()

    def reset_to_start_of_trajectory(self):
        # if "-" in self.curr_traj_id:
            # seed = int(self.curr_traj_id.split("-")[0])
        # else:
            # seed = int(self.curr_traj_id)
        # import ipdb;ipdb.set_trace()
        if "env_init_state" in self.trajectory:
            
            self.env.reset(seed=int(self.curr_traj_id), reconfigure=False)
            self.env.set_state(self.trajectory["env_init_state"])
        else:
            self.env.reset(seed=int(self.curr_traj_id), reconfigure=False)
        if self.goal == "pick_and_place_silo":
            qpos = np.array(
                [0, np.pi / 16, 0, -np.pi * 5 / 6, 0, np.pi - 0.2, np.pi / 4, 0, 0]
            )

            self.env.agent.reset(qpos)
    def draw_teacher_trajectory(self, skip=4, **kwargs):
        for ball in self.teacher_balls:
            self.env.scene.remove_actor(ball)
        self.teacher_balls = []
        obs = self.orig_trajectory_observations[:-1:skip+1]
        obs = np.vstack([obs, self.orig_trajectory_observations[-1]])
        for i, o in enumerate(obs):
            frac = i/len(obs)
            # pose = sapien.Pose([o[0], o[1], o[2]])
            pose = sapien.Pose([o[3], o[4], o[5]])            
            ball = add_target(
                self.env.scene,
                pose=pose,
                radius=0.0075,
                color=[(159/255)*frac,(51/255)*frac,frac*214/255],
                target_id=f"shadow_{i}",
            )
            self.teacher_balls.append(ball)
    def _plan_trajectory(self, start_state, start_obs):
        # if self.env.intervene_count > 0:
            # make empty step to allow for intervention before we plan again
        self.env.step(self.env.action_space.sample() *0)
        start_state = self.env.get_state()
        self.planning_env: BlockStackMagicPandaEnv
        self.planning_env.allow_next_stage(False)

        self.planning_env.reset()
        self.planning_env.set_state(start_state)
        # import pdb;pdb.set_trace()
        assert self.planning_env.obs_mode == 'state_dict'
        obs = dict(
            is_grasped = self.planning_env.connected,
            planning_env = self.planning_env
        )
        try:
            observations = self.planner.generate_magic_teacher_traj_for_env(obs, render=self.planner_cfg["render_plan"], save_video=self.planner_cfg['save_plan_videos'], plan_id=self.total_plans)
        except:
            print(self.curr_traj_id)
            observations = np.zeros((10,10))
        if self._traj_len(dict(observations=observations)) > self.max_trajectory_length:
            observations = np.zeros((10, 10))
        self.env.step(np.zeros(4))
        return {
            "observations": np.array(observations),
            "env_init_state": start_state
        }