import copy
from collections import OrderedDict, deque
from pathlib import Path
from typing import Dict, List, Tuple

ROOT_DIR = Path(__file__).parent.parent.resolve()
ASSET_DIR = ROOT_DIR / "assets"
AGENT_CONFIG_DIR = ASSET_DIR / "config_files/agents"
DESCRIPTION_DIR = ASSET_DIR / "descriptions"

import attr
import numpy as np
import sapien.core as sapien
import yaml
from gym import spaces
# from mani_skill2 import ASSET_DIR, DESCRIPTION_DIR
from mani_skill2.agents.active_light_sensor import ActiveLightSensor
from mani_skill2.agents.camera import get_camera_images, get_camera_pcd
from mani_skill2.agents.controllers.combined_controller import CombinedController
from mani_skill2.utils.sapien_utils import get_entity_by_name, make_actor_visible


@attr.s(auto_attribs=True, kw_only=True)
class MountedCameraConfig:
    name: str
    # extrinsic parameters
    mount: str  # name of link to mount
    mount_p: List[float]  # position relative to link
    mount_q: List[float]  # quaternion relative to link
    hide_mount_link: bool  # whether to hide the mount link
    # intrinsic parameters
    width: int
    height: int
    near: float
    far: float
    fx: float
    fy: float
    cx: float
    cy: float
    skew: float


@attr.s(auto_attribs=True, kw_only=True)
class MountedActiveLightSensorConfig:
    name: str
    # extrinsic parameters
    mount: str  # name of link to mount
    # intrinsic parameters
    rgb_resolution: Tuple[int, int]
    ir_resolution: Tuple[int, int]
    rgb_intrinsic: List[float]
    ir_intrinsic: List[float]
    trans_pose_l: List[float]
    trans_pose_r: List[float]
    light_pattern_path: str
    max_depth: float
    min_depth: float
    ir_ambient_strength: float
    ir_light_dim_factor: float
    ir_light_fov: float


def create_mounted_camera(
    config: MountedCameraConfig, robot: sapien.Articulation, scene: sapien.Scene
) -> sapien.CameraEntity:
    camera_mount = get_entity_by_name(robot.get_links(), config.mount)
    mount_pose = sapien.Pose(config.mount_p, config.mount_q)
    camera = scene.add_mounted_camera(
        config.name,
        camera_mount,
        mount_pose,
        width=config.width,
        height=config.height,
        fovy=0,  # focal will be set later.
        near=config.near,
        far=config.far,
    )
    camera.set_perspective_parameters(
        near=config.near,
        far=config.far,
        fx=config.fx,
        fy=config.fy,
        cx=config.cx,
        cy=config.cy,
        skew=config.skew,
    )
    if config.hide_mount_link:
        make_actor_visible(camera_mount, False)
    return camera


def create_mounted_sensor(
    config: MountedActiveLightSensorConfig,
    robot: sapien.Articulation,
    scene: sapien.Scene,
) -> ActiveLightSensor:
    sensor_mount = get_entity_by_name(robot.get_links(), config.mount)
    rgb_intrinsic = np.array(config.rgb_intrinsic).reshape(3, 3).astype(np.float)
    ir_intrinsic = np.array(config.ir_intrinsic).reshape(3, 3).astype(np.float)
    trans_pose_l = sapien.Pose(config.trans_pose_l[:3], config.trans_pose_l[3:])
    trans_pose_r = sapien.Pose(config.trans_pose_r[:3], config.trans_pose_r[3:])

    sensor = ActiveLightSensor(
        name=config.name,
        scene=scene,
        mount=sensor_mount,
        rgb_resolution=config.rgb_resolution,
        ir_resolution=config.ir_resolution,
        rgb_intrinsic=rgb_intrinsic,
        ir_intrinsic=ir_intrinsic,
        trans_pose_l=trans_pose_l,
        trans_pose_r=trans_pose_r,
        light_pattern=str(ASSET_DIR / config.light_pattern_path),
        max_depth=config.max_depth,
        min_depth=config.min_depth,
        ir_ambient_strength=config.ir_ambient_strength,
        ir_light_dim_factor=config.ir_light_dim_factor,
        ir_light_fov=config.ir_light_fov,
    )
    return sensor


@attr.s(auto_attribs=True, kw_only=True)
class AgentConfig:
    agent_class: str
    name: str
    urdf_file: str
    fix_root_link: bool = True
    urdf_config: Dict = attr.ib(factory=dict)

    default_init_qpos: List[float]
    default_control_mode: str

    controllers: Dict[str, List[Dict]]
    cameras: List[Dict] = attr.ib(factory=list)
    sensors: List[Dict] = attr.ib(factory=list)

    torque_freq: int


def parse_urdf_config(config_dict: Dict, scene: sapien.Scene) -> Dict:
    """Parse config from dict for SAPIEN URDF loader.

    Args:
        config_dict (Dict): dict containing link physical properties.
        scene (sapien.Scene): simualtion scene

    Returns:
        Dict: urdf config passed to `sapien.URDFLoader.load`.
    """
    config_dict = copy.deepcopy(config_dict)
    urdf_config = {}

    phy_mtls = {}
    for k, v in config_dict.get("materials", {}).items():
        phy_mtls[k] = scene.create_physical_material(**v)

    link_configs = config_dict.get("links", {})
    for link_name, link_config in link_configs.items():
        # replace with actual material
        link_config["material"] = phy_mtls[link_config["material"]]

    urdf_config["link"] = link_configs
    return urdf_config


class BaseAgent:
    """Base class for agents.

    Agent is an interface of the robot (sapien.Articulation).

    Args:
        config (AgentConfig):  agent configuration.
        scene (sapien.Scene): simulation scene instance.
        control_freq (int): control frequency (Hz).

    """

    _config: AgentConfig
    _scene: sapien.Scene
    _robot: sapien.Articulation
    _combined_controllers: Dict[str, CombinedController]
    _control_mode: str
    _cameras: Dict[str, sapien.CameraEntity]
    _sensors: Dict[str, ActiveLightSensor]
    _q_vel_ex: np.ndarray  # cached q velocity of previous step for external torque
    _sim_time_step: float

    def __init__(self, config: AgentConfig, scene: sapien.Scene, control_freq: int):
        self._config = copy.deepcopy(config)

        self._scene = scene
        self._sim_time_step = scene.timestep
        self._control_freq = control_freq
        assert control_freq % config.torque_freq == 0
        torque_queue_len = control_freq // config.torque_freq
        self._torque_queue = deque(maxlen=torque_queue_len)

        self._initialize_robot()
        self._initialize_controllers()
        self._initialize_cameras()
        self._initialize_sensors()

    def _initialize_robot(self):
        loader = self._scene.create_urdf_loader()
        loader.fix_root_link = self._config.fix_root_link

        urdf_file = DESCRIPTION_DIR / self._config.urdf_file
        urdf_config = parse_urdf_config(self._config.urdf_config, self._scene)
        self._robot = loader.load(str(urdf_file), urdf_config)
        self._robot.set_name(self._config.name)

    def _initialize_controllers(self):
        self._combined_controllers = OrderedDict()
        for control_mode, controller_configs in self._config.controllers.items():
            self._combined_controllers[control_mode] = CombinedController(
                controller_configs, self._robot, self._control_freq
            )
        self._control_mode = self._config.default_control_mode
        self._combined_controllers[self._control_mode].set_joint_drive_property()

    def _initialize_cameras(self):
        self._cameras = OrderedDict()
        for config in self._config.cameras:
            config = MountedCameraConfig(**config)
            if config.name in self._cameras:
                raise KeyError("Non-unique camera name: {}".format(config.name))
            cam = create_mounted_camera(config, self._robot, self._scene)
            self._cameras[config.name] = cam

    def _initialize_sensors(self):
        self._sensors = OrderedDict()
        for config in self._config.sensors:
            config = MountedActiveLightSensorConfig(**config)
            if config.name in self._sensors:
                raise KeyError(
                    "Non-unique active light sensor name: {}".format(config.name)
                )
            sensor = create_mounted_sensor(config, self._robot, self._scene)
            self._sensors[config.name] = sensor

    @classmethod
    def from_config_file(cls, config_path: str, scene: sapien.Scene, control_freq: int):
        with open(config_path, "r") as f:
            config_dict = yaml.safe_load(f)
        config = AgentConfig(**config_dict)
        return cls(config, scene, control_freq)

    @property
    def supported_control_modes(self) -> Tuple[str]:
        return tuple(self._combined_controllers.keys())

    @property
    def control_mode(self) -> str:
        return self._control_mode

    def set_control_mode(self, control_mode: str):
        assert (
            control_mode in self.supported_control_modes
        ), f"Unknown control mode: [{control_mode}]. All supported control modes: [{self.supported_control_modes}]"
        self._control_mode = control_mode
        self._combined_controllers[control_mode].set_joint_drive_property()
        self._combined_controllers[control_mode].reset()

    @property
    def action_space(self):
        return spaces.Dict(
            {
                mode: spaces.Box(
                    controller.action_range[:, 0], controller.action_range[:, 1]
                )
                for mode, controller in self._combined_controllers.items()
            }
        )

    @property
    def action_range(self) -> spaces.Box:
        return spaces.Box(
            self._combined_controllers[self._control_mode].action_range[:, 0],
            self._combined_controllers[self._control_mode].action_range[:, 1],
        )

    def set_action(self, action: np.ndarray):
        self._combined_controllers[self._control_mode].set_action(action)

    def simulation_step(self):
        self._combined_controllers[self._control_mode].simulation_step()
        self._q_vel_ex = self._robot.get_qvel()

    def reset(self, init_qpos=None):
        if init_qpos is None:
            init_qpos = self._config.default_init_qpos
        self._robot.set_qpos(init_qpos)
        self._robot.set_qvel(np.zeros(self._robot.dof))
        self._robot.set_qacc(np.zeros(self._robot.dof))
        self._robot.set_qf(np.zeros(self._robot.dof))
        self._combined_controllers[self._control_mode].reset()
        self._q_vel_ex = self._robot.get_qvel()
        self._torque_queue.clear()

    def get_proprioception(self) -> Dict:
        raise NotImplementedError

    def get_state(self) -> Dict:
        """Get current state for MPC, including robot state and controller state"""
        state = OrderedDict()

        # robot state
        root_link = self._robot.get_links()[0]
        state["robot_root_pose"] = root_link.get_pose()
        state["robot_root_vel"] = root_link.get_velocity()
        state["robot_root_qvel"] = root_link.get_angular_velocity()
        state["robot_qpos"] = self._robot.get_qpos()
        state["robot_qvel"] = self._robot.get_qvel()
        state["robot_qacc"] = self._robot.get_qacc()

        # controller state
        # TODO

        return state

    def set_state(self, state: Dict):
        # robot state
        self._robot.set_root_pose(state["robot_root_pose"])
        self._robot.set_root_velocity(state["robot_root_vel"])
        self._robot.set_root_angular_velocity(state["robot_root_qvel"])
        self._robot.set_qpos(state["robot_qpos"])
        self._robot.set_qvel(state["robot_qvel"])
        self._robot.set_qacc(state["robot_qacc"])

    def get_fused_pointcloud(
        self, rgb=True, visual_seg=False, actor_seg=False
    ) -> Dict[str, np.ndarray]:
        """get pointcloud from each camera, transform them to the *world* frame, and fuse together"""
        assert self._cameras
        pcds = []
        # self._scene.update_render()
        for cam in self._cameras.values():
            cam.take_picture()
            pcd = get_camera_pcd(cam, rgb, visual_seg, actor_seg)  # dict
            # Model matrix is the transformation from OpenGL camera space to SAPIEN world space
            # camera.get_model_matrix() must be called after scene.update_render()!
            T = cam.get_model_matrix()
            R = T[:3, :3]
            t = T[:3, 3]
            pcd["xyz"] = pcd["xyz"] @ R.transpose() + t
            pcds.append(pcd)
        fused_pcd = {}
        for key in pcds[0].keys():
            fused_pcd[key] = np.concatenate([pcd[key] for pcd in pcds], axis=0)
        return fused_pcd

    def get_images(
        self, rgb=True, depth=False, visual_seg=False, actor_seg=False
    ) -> Dict[str, Dict[str, np.ndarray]]:
        assert self._cameras
        all_images = OrderedDict()
        # self._scene.update_render()
        for cam_name, cam in self._cameras.items():
            cam.take_picture()
            cam_images = get_camera_images(
                cam, rgb=rgb, depth=depth, visual_seg=visual_seg, actor_seg=actor_seg
            )
            cam_images["camera_intrinsic"] = cam.get_intrinsic_matrix()
            cam_extrinsic_world_frame = cam.get_extrinsic_matrix()
            robot_base_frame = (
                self._robot.get_root_pose().to_transformation_matrix()
            )  # robot base -> world
            cam_images["camera_extrinsic_base_frame"] = (
                cam_extrinsic_world_frame @ robot_base_frame
            )  # robot base -> camera
            all_images[cam_name] = cam_images

        if depth:
            for sensor_name, sensor in self._sensors.items():
                all_images[sensor_name] = sensor.get_image_dict()

        return all_images

    def get_camera_poses(self) -> Dict[str, np.ndarray]:
        assert self._cameras
        pose_list = OrderedDict()
        for cam_name, cam in self._cameras.items():
            pose_list[cam_name] = cam.get_pose().to_transformation_matrix()
        return pose_list

    def update_generalized_external_forces(self):
        """Note: only called in step_action()"""
        q_pos = self._robot.get_qpos()
        q_vel = self._robot.get_qvel()
        actual_acc = (q_vel - self._q_vel_ex) / self._sim_time_step
        actual_torques = self._robot.compute_inverse_dynamics(actual_acc)
        command_torques = []
        for j_idx, j in enumerate(self._robot.get_active_joints()):
            command_torque = j.stiffness * (
                j.get_drive_target() - q_pos[j_idx]
            ) + j.damping * (j.get_drive_velocity_target() - q_vel[j_idx])
            command_torques.append(command_torque)

        command_torques = np.array(command_torques)
        F = (
            actual_torques
            - command_torques
            - self._robot.get_qf()
            + self._robot.compute_passive_force()
        )
        self._torque_queue.append(F)

    def get_generalized_external_forces(self):
        if self._torque_queue:
            return np.mean(np.stack(self._torque_queue), axis=0)
        else:
            return np.zeros_like(self._robot.get_qf())
